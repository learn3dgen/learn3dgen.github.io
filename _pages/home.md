---
layout: project
urltitle:  "Learning 3D Generative Models"
title: "Learning 3D Generative Models"
categories: cvpr, workshop, computer vision, computer graphics, deep learning, generative modeling, visual learning, simulation environments, robotics, machine learning, reinforcement learning
permalink: /
favicon: /static/img/ico/favicon.png
bibtex: true
paper: true
acknowledgements: ""
---

<br>
<div class="row header-row">
  <div class="col-xs-12 header-img">  
    <center><h1>Learning 3D Generative Models</h1></center>
    <center><h2>CVPR 2020 Workshop, Seattle, WA</h2></center>
    <center><span style="font-weight:400;">Time and Location TBD</span></center>
    <!--<center><span style="color:#e74c3c;font-weight:400;">Time and Location TBD</span></center>-->
    <br/>
  </div>
</div>

<hr>

<!-- <div class="row" id="">
  <div class="col-md-12">
    <img src="{{ "/static/img/splash.png" | prepend:site.baseurl }}">
    <p> Image credit: [1, 2, 7, 12, 6, 4, 5]</p>
  </div>
</div> -->

<br>
<div class="row" id="intro">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      The past several years have seen an explosion of interest in generative modeling: unsupervised models which learn to synthesize new elements from the training data domain. Such models have been used to breathtaking effect for generating realistic images, especially of human faces, which are in some cases indistinguishable from reality. The unsupervised latent representations learned by these models can also prove powerful when used as feature sets for supervised learning tasks.
    </p>
    <p>
      Thus far, the vision community's attention has mostly focused on generative models of 2D images. However, in computer graphics, there has been a recent surge of activity in generative models of three-dimensional content: learnable models which can synthesize novel 3D objects, or even larger scenes composed of multiple objects. As the vision community turns from passive internet-images based vision toward more <i>embodied</i> vision tasks, these kinds of 3D generative models become increasingly important: as unsupervised feature learners, as training data synthesizers, as a platform to study 3D representations for 3D vision tasks, and as a way of equipping an embodied agent with a 3D `imagination' about the kinds of objects and scenes it might encounter.
    </p>
    <p>
     With this workshop, we aim to bring together researchers working on generative models of 3D shapes and scenes with researchers and practitioners who can use these generative models to improve embodied vision tasks. For our purposes, we define ``generative model'' to include methods that synthesize geometry unconditionally as well as from sensory inputs (e.g. images), language, or other high-level specifications. Vision tasks that can benefit from such models include scene classification and segmentation, 3D reconstruction, human activity recognition, robotic visual navigation, question answering, and more.
    </p>
  </div>
</div> <br>   

<div class="row" id="cfp">
  <div class="col-xs-12">
    <h2>Call for Papers</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      <span style="font-weight:500;">Call for papers:</span> We invite extended abstracts for work on tasks related to data-driven 3D generative modeling or tasks leveraging generated 3D content.
      Paper topics may include but are not limited to:
    </p>
    <ul>
      <li>Generative models for 3D shape and 3D scene synthesis</li>
      <li>Generating 3D shapes and scenes from real world data (images, videos, or scans)</li>
      <li>Representations for 3D shapes and scenes</li>
      <li>Unsupervised feature learning for embodied vision tasks via 3D generative models</li>
      <li>Training data synthesis/augmentation for embodied vision tasks via 3D generative models</li>
    </ul>
    <p>
      <span style="font-weight:500;">Submission:</span> we encourage submissions of up to 6 pages excluding references and acknowledgements.
      The submission should be in the CVPR format.
      Reviewing will be single blind.
      Accepted extended abstracts will be made publicly available as non-archival reports, allowing future submissions to archival conferences or journals.
      We also welcome already published papers that are within the scope of the workshop (without re-formatting), including papers from the main CVPR conference.
      The submissions will be handled by the CMT paper management system (available shortly). Please mention in your submission if your submission has already been accepted for publication (and the name of the conference).
      <!--Please submit your paper to the following address by the deadline: <span style="color:#1a1aff;font-weight:400;"><a href="mailto:learn3dgen@gmail.com">learn3dgen@gmail.com</a></span>-->
    </p>
  </div>
</div><br>

<div class="row" id="dates">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped">
      <tbody>
        <tr>
          <td>Paper Submission Deadline</td>
          <td>May 17 2020</td>
        </tr>
        <tr>
          <td>Notification to Authors</td>
          <td>May 31 2020</td>
        </tr>
        <tr>
          <td>Camera-Ready Deadline</td>
          <td>June 7 2020</td>
        </tr>
        <tr>
          <td>Workshop Date</td>
          <td>TBD, one of June 14,15,19 2020</td>
        </tr>
      </tbody>
    </table>
  </div>
</div><br>


<div class="row" id="schedule">
  <div class="col-xs-12">
    <h2>Schedule</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
     <table class="table table-striped">
      <tbody>
        <tr>
          <td>Welcome and Introduction</td>
          <td>8:45am - 9:00am</td>
        </tr>
        <tr>
          <td>Invited Talk 1</td>
          <td>9:00am - 9:25am</td>
        </tr>
        <tr>
          <td>Invited Talk 2</td>
          <td>9:25am - 9:50am</td>
        </tr>
        <tr>
          <td>Spotlight Talks (x3)</td>
          <td>9:50am - 10:05am</td>
        </tr>
        <tr>
          <td>Coffee Break and Poster Session</td>
          <td>10:05am - 11:00am</td>
        </tr>
        <tr>
          <td>Invited Talk 3</td>
          <td>11:00am - 11:25am</td>
        </tr>
        <tr>
          <td>Invited Talk 4</td>
          <td>11:25am - 11:50am</td>
        </tr>
        <tr>
          <td>Lunch Break</td>
          <td>11:50am - 1:00pm</td>
        </tr>
        <tr>
          <td>Invited Talk 5</td>
          <td>1:00pm - 2:40pm</td>
        </tr>
        <tr>
          <td>Spotlight Talks (x3)</td>
          <td>2:40pm - 3:05pm</td>
        </tr>
        <tr>
          <td>Coffee Break and Poster Session</td>
          <td>3:05pm - 4:00pm</td>
        </tr>
        <tr>
          <td>Invited Talk 6</td>
          <td>4:00pm - 4:25pm</td>
        </tr>
        <tr>
          <td>Invited Talk 7</td>
          <td>4:25pm - 4:50pm</td>
        </tr>
        <tr>
          <td>Invited Talk 8</td>
          <td>4:50pm - 5:15pm</td>
        </tr>
        <tr>
          <td>Panel Discussion and Conclusion</td>
          <td>5:15pm - 6:00pm</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<br>
<div class="row" id="accepted">
  <div class="col-md-12">
    <h2>Accepted Papers</h2>
  </div>
</div>

<span style="color:#e74c3c;font-weight:400;">TBD</span>

<!--
<br>
<div class="row">
  <div class="col-md-12">
    <span style="font-weight:200;">DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation</span><br>
    Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, Steven Lovegrove<br>
    <a href="https://drive.google.com/file/d/1oYUQOicJ-gIcLaQUnrE23azPDMW5VphP/view?usp=sharing">Paper</a> | Poster #24 AM (morning)
  </div>
</div>
 -->

<br>
<div class="row" id="speakers">
  <div class="col-xs-12">
    <h2>Invited Speakers</h2>
  </div>
</div><br>



<!-- 70 -->
<div class="row">
  <div class="col-md-12">
    <a href="http://www.cs.utexas.edu/users/grauman/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/speakers/grauman.jpg" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="http://www.cs.utexas.edu/users/grauman/">Kristen Grauman</a></b> is a Professor in the Department of Computer Science at the University of Texas at Austin and a Research Scientist in Facebook AI Research (FAIR).  Her research in computer vision and machine learning focuses on visual recognition and search.  Before joining UT-Austin in 2007, she received her Ph.D. at MIT.  She is an Alfred P. Sloan Research Fellow and Microsoft Research New Faculty Fellow, a recipient of NSF CAREER and ONR Young Investigator awards, the PAMI Young Researcher Award in 2013, the 2013 Computers and Thought Award from the International Joint Conference on Artificial Intelligence (IJCAI), the Presidential Early Career Award for Scientists and Engineers (PECASE) in 2013, and the Helmholtz Prize in 2017. 
    </p>
  </div>
</div><br>

<!-- 60 -->
<div class="row">
  <div class="col-md-12">
    <a href="http://vladlen.info/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/speakers/vladlen.png" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="http://vladlen.info/">Vladlen Koltun</a></b> is a Senior Principal Researcher and the director of the Intelligent Systems Lab at Intel. The lab is devoted to high-impact basic research on intelligent systems. Previously, he has been a Senior Research Scientist at Adobe Research and an Assistant Professor at Stanford where his theoretical research was recognized with the National Science Foundation (NSF) CAREER Award (2006) and the Sloan Research Fellowship (2007).
    </p>
  </div>
</div><br>

<!-- 47 -->
<div class="row">
  <div class="col-md-12">
    <a href="https://www.cs.utoronto.ca/~fidler/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/speakers/sanja.jpg" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://www.cs.utoronto.ca/~fidler/">Sanja Fidler</a></b> is an Assistant Professor at University of Toronto, and a Director of AI at NVIDIA, leading a research lab in Toronto. Prior coming to Toronto, in 2012/2013, she was a Research Assistant Professor at Toyota Technological Institute at Chicago, an academic institute located in the campus of University of Chicago. She did her postdoc with Prof. Sven Dickinson at University of Toronto in 2011/2012. She finished her PhD in 2010 at University of Ljubljana in Slovenia in the group of Prof. Ales Leonardis. In 2010, she was visiting Prof. Trevor Darrell's group at UC Berkeley and ICSI. 
    </p>
  </div>
</div><br>

<!-- 36 -->
<div class="row">
  <div class="col-md-12">
    <a href="https://www.cs.purdue.edu/homes/aliaga/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/speakers/aliaga.png" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://www.cs.purdue.edu/homes/aliaga/">Daniel Aliaga</a></b> does research primarily in the area of 3D computer graphics but overlaps with computer vision and visualization while also having strong multi-disciplinary collaborations outside of computer science. His research activities are divided into three groups: a) his pioneering work in the multi-disciplinary area of inverse modeling and design; b) his first-of-its-kind work in codifying information into images and surfaces, and c) his compelling work in a visual computing framework including high-quality 3D acquisition methods. Dr. Aliaga’s inverse modeling and design is particularly focused at digital city planning applications that provide innovative “what-if” design tools enabling urban stake holders from cities worldwide to automatically integrate, process, analyze, and visualize the complex interdependencies between the urban form, function, and the natural environment.
    </p>
  </div>
</div><br>

<!-- 24 -->
<div class="row">
  <div class="col-md-12">
    <a href="https://jiajunwu.com/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/speakers/jiajun.png" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://jiajunwu.com/">Jiajun Wu</a></b> is a fifth-year PhD student at MIT, advised by Bill Freeman and Josh Tenenbaum. He received his undergraduate degree from Tsinghua University, working with Zhuowen Tu. He has also spent time at research labs of Microsoft, Facebook, and Baidu. His research has been supported by fellowships from Facebook, Nvidia, Samsung, Baidu, and Adobe. He studies machine perception, reasoning, and its interaction with the physical world, drawing inspiration from human cognition.
    </p>
  </div>
</div><br>


<!-- 16 -->
<div class="row">
  <div class="col-md-12">
    <a href=""><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/speakers/georgia.jpg" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="">Georgia Gkioxari</a></b> is a research scientist at FAIR. She received her PhD from UC Berkeley, where she was advised by Jitendra Malik. She did her bachelors in ECE at NTUA in Athens, Greece, where she worked with Petros Maragos. In the past, she has spent time at Google Brain and Google Research, where she worked with Navdeep Jaitly and Alexander Toshev.
    </p>
  </div>
</div><br>


<!-- 16 -->
<div class="row">
  <div class="col-md-12">
    <a href="https://demuc.de/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/speakers/johannes.jpg" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://demuc.de/">Johannes L. Schönberger</a></b> is a Senior Scientist at the Microsoft Mixed Reality and AI lab in Zürich. He obtained his PhD in Computer Science in the Computer Vision and Geometry Group at ETH Zürich, where he was advised by Marc Pollefeys and co-advised by Jan-Michael Frahm. He received a BSc from TU Munich and an MSc from UNC Chapel Hill. In addition, he also spent time at Microsoft Research, Google, and the German Aerospace Center. His main research interests lie in robust image-based 3D modeling. More broadly, he is interested in computer vision, geometry, structure-from-motion, (multi-view) stereo, localization, optimization, machine learning, and image processing. He developed the open-source software COLMAP - an end-to-end image-based 3D reconstruction software, which achieves state-of-the-art results on recent reconstruction benchmarks.
    </p>
  </div>
</div><br>


<!-- 15 -->
<div class="row">
  <div class="col-md-12">
    <a href="https://www.cse.iitb.ac.in/~sidch/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/speakers/sid.png" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://www.cse.iitb.ac.in/~sidch/">Siddhartha Chaudhuri</a></b> is a Senior Research Scientist at Adobe Research, and Assistant Professor (on leave) of Computer Science and Engineering at IIT Bombay. His research focuses on richer tools for designing three-dimensional objects, particularly by novice and casual users, and on related problems in 3D shape understanding, synthesis and reconstruction. He received his PhD from Stanford University, followed by a postdoc at Princeton and a year teaching at Cornell. Apart from basic research, he is also the original author of the commercial 3D modeling package Adobe Fuse.
    </p>
  </div>
</div><br>

<!-- 13 -->
<div class="row">
  <div class="col-md-12">
    <a href="http://graphics.stanford.edu/~adai/"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/speakers/angela.jpg" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="http://graphics.stanford.edu/~adai/">Angela Dai</a></b> is a postdoctoral researcher at the Technical University of Munich.  She received her Ph.D. in Computer Science at Stanford University advised by Pat Hanrahan. Her research focuses on 3D reconstruction and understanding with commodity sensors. She received her Masters degree from Stanford University and her Bachelors degree from Princeton University. She is a recipient of a Stanford Graduate Fellowship.
    </p>
  </div>
</div><br>

<!-- 13 -->
<div class="row">
  <div class="col-md-12">
    <a href="https://cs.brown.edu/people/epavlick/index.html"><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/speakers/ellie.gif" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="https://cs.brown.edu/people/epavlick/index.html">Ellie Pavlick</a></b> is an Assistant Professor of Computer Science at Brown University, and an academic partner with Google AI. She received her PhD in Computer Science from the University of Pennsylvania. She is interested in building better computational models of natural language semantics and pragmatics: how does language work, and how can we get computers to understand it the way humans do?
    </p>
  </div>
</div><br>













<!--
<div class="row">
  <div class="col-md-12">
    <a href=""><img class="people-pic" style="float:left;margin-right:50px;" src="{{ "/static/img/speakers/" | prepend:site.baseurl }}"></a>
    <p>
      <b><a href="">ABC DEF</a></b> is a .
    </p>
  </div>
</div><br>
-->










<!-- <div class="row">
  <div class="col-md-12">
    <b>Industry Participants</b>
    <p>The workshop also features presentations by representatives of the following companies:</p>
  </div>
</div>
<div class="row">
  <div class="col-md-3">
    <a href="https://planner5d.com/"><img src="/static/img/p5d.png" /></a>
  </div>
  <div class="col-md-3">
    <a href="https://wayfair.com/"><img src="/static/img/wayfair.png" /></a>
  </div>
  <div class="col-md-3">
    <a href="https://modsy.com/"><img src="/static/img/modsy.png" /></a>
  </div>
  <div class="col-md-3">
    <a href="https://zillow.com/"><img src="/static/img/zillow.png" /></a>
  </div>
</div><br> -->


<div class="row" id="organizers">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>

<div class="row">

  <div class="col-xs-2">
    <a href="https://angelxuanchang.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/angel.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://angelxuanchang.github.io/">Angel X. Chang</a>
      <h6>Simon Fraser University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://dritchie.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/daniel.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://dritchie.github.io/">Daniel Ritchie</a>
      <h6>Brown University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://www.cs.utexas.edu/~huangqx/">
      <img class="people-pic" src="{{ "/static/img/people/qixing.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cs.utexas.edu/~huangqx/">Qixing Huang</a>
      <h6>UT Austin</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="http://msavva.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/manolis.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://msavva.github.io/">Manolis Savva</a>
      <h6>Simon Fraser University</h6>
    </div>
  </div>


  <div class="col-xs-2">
    <a href="https://fgolemo.github.io/">
      <img class="people-pic" src="{{ "/static/img/people/florian.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://fgolemo.github.io/">Florian Golemo</a>
      <h6>MILA, Element AI</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://mila.quebec/en/person/sai-rajeshwar/">
      <img class="people-pic" src="{{ "/static/img/people/sai.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://mila.quebec/en/person/sai-rajeshwar/">Sai Rajeswar</a>
      <h6>MILA</h6>
    </div>
  </div>

</div>

<div class="row">



  <div class="col-xs-2">
    <a href="http://www.david-vazquez.com/">
      <img class="people-pic" src="{{ "/static/img/people/david.jpeg" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://www.david-vazquez.com/">David Vazquez</a>
      <h6>Element AI</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="http://www.pedro.opinheiro.com/">
      <img class="people-pic" src="{{ "/static/img/people/pedro.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="http://www.pedro.opinheiro.com/">Pedro O. Pinheiro</a>
      <h6>Element AI</h6>
    </div>
  </div>
  
  
  <div class="col-xs-2">
    <a href="https://www.cse.iitb.ac.in/~sidch/">
      <img class="people-pic" src="{{ "/static/img/people/sid.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cse.iitb.ac.in/~sidch/">Siddhartha Chaudhuri</a>
      <h6>Adobe Research, IIT Bombay</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://kevinkaixu.net/">
      <img class="people-pic" src="{{ "/static/img/people/kevin.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://kevinkaixu.net/">Kai (Kevin) Xu</a>
      <h6>NUDT</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://www.cs.sfu.ca/~haoz/">
      <img class="people-pic" src="{{ "/static/img/people/richard.png" | prepend:site.baseurl }}">
    </a>
    <div class="people-name">
      <a href="https://www.cs.sfu.ca/~haoz/">Hao (Richard) Zhang</a>
      <h6>Simon Fraser University</h6>
    </div>
  </div>
</div>


<hr>

{% if page.acknowledgements %}
<div class="row">
  <div class="col-xs-12">
    <h2>Acknowledgments</h2>
  </div>
</div>
<a name="/acknowledgements"></a>
<div class="row">
  <div class="col-xs-12">
    <p>
      Thanks to <span style="color:#1a1aff;font-weight:400;"> <a href="https://visualdialog.org/">visualdialog.org</a></span> for the webpage format.
    </p>
  </div>
</div>
{% endif %}

<br>

<!--

<div class="row">
  <div class="col-xs-12">
    <h2>References</h2>
  </div>
</div>

{:.paper}
<span>[1] Fast and Flexible Indoor Scene Synthesis via Deep Convolutional Generative Models</span>{:.papertitle}  
<span>D. Ritchie, K. Wang, and Y.a. Lin</span>{:.authors}  
<span>_CoRR_, vol. arXiv:1811.12463, 2018</span>{:.journal}  

{:.paper}
<span>[2] GRAINS: Generative Recursive Autoencoders for INdoor Scenes</span>{:.papertitle}  
<span>M. Li, A.G. Patil, K. Xu, S. Chaudhuri, O. Khan, A. Shamir, C. Tu, B. Chen, D. Cohen-Or, and H. Zhang</span>{:.authors}  
<span>_CoRR_, vol. arXiv:1807.09193, 2018</span>{:.journal}  

{:.paper}
<span>[3] Gibson env: real-world perception for embodied agents</span>{:.papertitle}  
<span>F. Xia, A. R. Zamir, Z.Y. He, A. Sax, J. Malik, and S. Savarese</span>{:.authors}  
<span>Computer Vision and Pattern Recognition (CVPR), 2018 IEEE Conference on, IEEE, 2018</span>{:.journal}  

{:.paper}
<span>[4] VirtualHome: Simulating Household Activities via Programs</span>{:.papertitle}  
<span>X. Puig, K. Ra, M. Boben, J. Li, T. Wang, S. Fidler, and A. Torralba</span>{:.authors}  
<span>CVPR, 2018</span>{:.journal}  

{:.paper}
<span>[5] Embodied Question Answering</span>{:.papertitle}  
<span>A. Das, S. Datta, G. Gkioxari, S. Lee, D. Parikh, and D. Batra</span>{:.authors}  
<span>CVPR, 2018</span>{:.journal}  

{:.paper}
<span>[6] ScanComplete: Large-Scale Scene Completion and Semantic Segmentation for 3D Scans</span>{:.papertitle}  
<span>A. Dai, D. Ritchie, M. Bokeloh, S. Reed, J. Sturm, and M. Nießner</span>{:.authors}  
<span>Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2018</span>{:.journal}  

{:.paper}
<span>[7] SeeThrough: Finding Objects in Heavily Occluded Indoor Scene Images</span>{:.papertitle}  
<span>N. Mitra, V. Kim, E. Yumer, M. Hueting, N. Carr, and P. Reddy</span>{:.authors}  
<span>2018 International Conference on 3D Vision (3DV), 2018</span>{:.journal}  

{:.paper}
<span>[8] Matterport3D: Learning from RGB-D Data in Indoor Environments</span>{:.papertitle}  
<span>A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner, M. Savva, S. Song, A. Zeng, and Y. Zhang</span>{:.authors}  
<span>_International Conference on 3D Vision (3DV)_, 2017</span>{:.journal}  

{:.paper}
<span>[9] Joint 2D-3D-semantic data for indoor scene understanding</span>{:.papertitle}  
<span>I. Armeni, S. Sax, A.R. Zamir, and S. Savarese</span>{:.authors}  
<span>_arXiv preprint arXiv:1702.01105_, 2017</span>{:.journal}  

{:.paper}
<span>[10] MINOS: Multimodal Indoor Simulator for Navigation in Complex Environments</span>{:.papertitle}  
<span>M. Savva, A.X. Chang, A. Dosovitskiy, T. Funkhouser, and V. Koltun</span>{:.authors}  
<span>_arXiv:1712.03931_, 2017</span>{:.journal}  

{:.paper}
<span>[11] AI2-THOR: An interactive 3D environment for visual AI</span>{:.papertitle}  
<span>E. Kolve, R. Mottaghi, D. Gordon, Y. Zhu, A. Gupta, and A. Farhadi</span>{:.authors}  
<span>_arXiv preprint arXiv:1712.05474_, 2017</span>{:.journal}  

{:.paper}
<span>[12] Physically-Based Rendering for Indoor Scene Understanding Using Convolutional Neural Networks</span>{:.papertitle}  
<span>Y. Zhang, S. Song, E. Yumer, M. Savva, J.Y. Lee, H. Jin, and T. Funkhouser</span>{:.authors}  
<span>_The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2017</span>{:.journal}  

{:.paper}
<span>[13] Semantic scene completion from a single depth image</span>{:.papertitle}  
<span>S. Song, F. Yu, A. Zeng, A.X. Chang, M. Savva, and T. Funkhouser</span>{:.authors}  
<span>Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), 2017</span>{:.journal}  

{:.paper}
<span>[14] ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes</span>{:.papertitle}  
<span>A. Dai, A.X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner</span>{:.authors}  
<span>Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017</span>{:.journal}  

{:.paper}
<span>[15]  CARLA: An Open Urban Driving Simulator</span>{:.papertitle}  
<span>A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun</span>{:.authors}  
<span>1--16, Proceedings of the 1st Annual Conference on Robot Learning, 2017</span>{:.journal}  

{:.paper}
<span>[16] SceneNN: A Scene Meshes Dataset with aNNotations</span>{:.papertitle}  
<span>B.S. Hua, Q.H. Pham, D.T. Nguyen, M.K. Tran, L.F. Yu, and S.K. Yeung</span>{:.authors}  
<span>International Conference on 3D Vision (3DV), 2016</span>{:.journal}   -->


